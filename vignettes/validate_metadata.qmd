---
title: "Validate Metadata"
author: "Kaelyn Long"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
    html:
        fontsize: 14px
        toc: true
        top-depth: 3
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(curatedMetagenomicDataCuration)
```

# Metadata Validation Workflow

`curatedMetagenomicDataCuration` has a number of metadata validation functions
that can be used to check curated data. The functions rely on a data dictionary
such as `inst/extdata/cMD_data_dictionary.csv`. To prepare for validation, load
in the dictionary as well as the metadata table(s) to be validated.

```{r load_data}
# Import cMD data dictionary
dictpath <- system.file("extdata", "cMD_data_dictionary.csv",
                        package = "curatedMetagenomicDataCuration")

dict <- read.csv(dictpath,
                 colClasses = "character")

# Load in test studies
curated_path <- system.file("curated", package = "curatedMetagenomicDataCuration")
study_names <- list.files(curated_path, pattern = "_metadata.tsv",
                       recursive = TRUE, full.names = TRUE)
studies <- lapply(study_names, function(x) read.delim(x, colClasses = "character"))
names(studies) <- gsub("_metadata.tsv", "", basename(study_names))
```

Once the data is loaded in, any of the following functions can be run with the
data dictionary and metadata table as input:

 * `check_required`: Confirms that required columns are present
 * `check_class`: Confirms that columns follow the specified class format
 * `check_unique`: Confirms that all values in a column are unique (when specified
by the data dictionary)
 * `check_allowed_values`: Confirms that all values match the specified regular
expression or fall into a specified set of allowed values

```{r validate_base}
# Choose single study
test_study <- studies[["ArtachoA_2021"]]

# Validating a single study
report <- dplyr::bind_rows(
  check_required(dict, test_study),
  check_class(dict, test_study),
  check_unique(dict, test_study),
  check_allowed_values(dict, test_study)
)
```

The report generated by each validation function is a table with the following
columns:

 * column: The column being validated
 * row: The row of the value being checked
 * value: The value being checked
 * check_type: Which checking function produced this report
 * expected: What was the expected value
 * valid: Is this value/column valid

Some of these columns don't exactly make sense for some checks (e.g. the row and
value columns in the output of `check_required`), but they are preserved with NA
values to allow the report to be combined with that of other functions.

The default behavior of these functions only includes noncompliant
columns/values in the report for improved readability. However, each of these
functions also accepts the argument `include_all = TRUE`, which will include all
columns/values in the report even when they are compliant.

```{r include_all}
# Demonstrate checking with include_all = TRUE
full_report <- dplyr::bind_rows(
  check_required(dict, test_study, include_all = TRUE),
  check_class(dict, test_study, include_all = TRUE),
  check_unique(dict, test_study, include_all = TRUE),
  check_allowed_values(dict, test_study, include_all = TRUE)
)
```

The functions can of course also be run on multiple metadata tables in sequence.

```{r multiple_studies, eval=FALSE}
# Run validation on all studies
check_all <- lapply(studies, function(x) {
  print(unique(x$study_name))
  report <- dplyr::bind_rows(
    check_required(dict, x),
    check_class(dict, x),
    check_unique(dict, x),
    check_allowed_values(dict, x)
  )
})
```

There is one final validation function, `check_dictionary_values`, which is an
expanded version of `check_allowed_values`. `check_dictionary_values` references
dictionary files in either OWL or CSV format that are stored in the
'gs://odm_template' Google Cloud Bucket. This will not validate based on regular
expression, but it will validate columns that use dynamic enums for the set of
allowed values.

The caveat of this function is that access to the `gs://odm_template` Google
Cloud Bucket is required. Authentication must be done with a service account
keyfile in the following manner:

```{r demo_auth, eval=FALSE}
googleCloudStorageR::gcs_auth("full/path/to/keyfile.json")
```

To obtain this keyfile, the owner of a service account affiliated with the
Google Cloud Project containing `gs://odm_template` must create it
according to the process detailed in the Google Cloud Guide
"[Create and delete service account keys](https://cloud.google.com/iam/docs/keys-create-delete)".

Additionally, the default bucket for `GoogleCloudStorageR` must be set to the
Google Bucket name, in this case `odm_template`. This is done with the following
command:

```{r demo_set_default_bucket, eval=FALSE}
googleCloudStorageR::gcs_global_bucket("odm_template")
```

Once authentication is set up, you will need to run the function
`cache_dictionaries` to download and cache the dictionary files. You can choose
whether to redownload files that are already cached, and specify a custom cache
directory if you would rather not use the default
(usually `~/.cache/R/curatedMetagenomicDataCuration`).

```{r cache_dicts, eval=FALSE}
cache_dictionaries(redownload = "ask", custom_cache = NULL)
```

Once setup is completed, `check_dictionary_values` is run the same way as
the other validation functions.

```{r check_dictionary, eval=FALSE}
dictionary_report <- check_dictionary_values(dict, test_study)
```

